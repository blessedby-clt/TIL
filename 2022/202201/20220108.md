# 공부한 내용

* 파이썬 머신러닝 완벽 가이드 인강 복습
  * numpy 복습
  * 신용카드 사기검출 데이터 실습
* 패스트캠퍼스 시계열 인터넷 강의 수강



# 새롭게 알게 된 내용

## numpy

* reshape(-1, 1) : 1차원 어레이를 2차원으로 만들어줄 때 사용.

* sort

  * np.sort : 원본 행렬 그대로 유지, 정렬된 행렬 반환

  * sort( _ ) : 원본 행렬 정렬한 형태로 변환

  * sort[::-1] : 내림차순 정렬

  * argsort

    ```python
    import numpy as np
    
    name_array = np.array['John', 'Mike', 'Sarah', 'Kate', 'Samuel']
    score_array = np.array[78, 95, 84, 98, 88]
    
    sort_indices_asc = np.argsort(score_array)
    
    print(name_array[sort_indices_asc])
    
    ## 결과값
    ['John', 'Sarah', 'Samuel', 'Mike', 'Kate']
    ```

    

## 앙상블 학습

![image-20220108222826172](C:\Users\user1\AppData\Roaming\Typora\typora-user-images\image-20220108222826172.png)



### (1) XGboost

* 예측성능이 뛰어남
* GBM 대신 빠른 성능
* 과적합 규제
* Tree Pruning

```python
xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=7,\
                        min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03)
```

* 하이퍼 파라미터
  * learning_rate : 0~1 사이의 값 지정, 부스팅 스텝을 반복적으로 수행할 때 업그레이드 되는 학습률 값
  * n_estimators : 약한 학습기의 개수(반복수행횟수)
  * max_child_weight : 결정트리의 min_child_leaft와 유사. 과적합 조절용
  * max_depth : 트리의 최대깊이
  * subsample : 트리가 커져서 과적합되는 것을 막기 위해 데이터를 샘플링하는 비율
  * reg_lamda : L2 규제 적용값. 과적합 제어
  * reg_alpha : L1 규제 적용값.
  * colsample_bytree : GBM의 max_feature 와 유사. 트리 생성에 필요한 피처를 임의로 샘플링하는데 사용. 매우 많은 피처가 있는 경우 과적합을 조정하는데 적용.
  * scale_pos_weight : 특정값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터.
  * gamma : 트리의 리프노드를 추가적으로 나눌지를 결정할 최소손실감소값. 해당 값보다 큰 손실(loss)이 감소된 경우에 리프 노드를 분리. 값이 클수록 과적합 감소 효과가 있음.



### (2) lightGBM

* 리프 중심 트리 분할

```python
from lightgbm import LGBMClassifier
# (boost_from_average가 True일 경우 레이블 값이 극도로 불균형 분포를 이루는 경우 재현률 및 ROC-AUC 성능이 매우 저하됨.)  
#  LightGBM 2.1.0 이상 버전에서 이와 같은 현상 발생 

lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

* 하이퍼 파라미터
  * learning_rate : 0~1 사이의 값 지정, 부스팅 스텝을 반복적으로 수행할 때 업그레이드 되는 학습률 값
  * n_estimators : 약한 학습기의 개수(반복수행횟수)
  * max_depth : 트리의 최대깊이
  * subsample : 트리가 커져서 과적합되는 것을 막기 위해 데이터를 샘플링하는 비율
  * colsample_bytree : GBM의 max_feature 와 유사. 트리 생성에 필요한 피처를 임의로 샘플링하는데 사용. 매우 많은 피처가 있는 경우 과적합을 조정하는데 적용.
  * min_child_samples : 리프 노드가 될 수 있는 데이터 건수(sample 수)
  * reg_lamda : L2 규제 적용값. 과적합 제어
  * reg_alpha : L1 규제 적용값.
  * early_stopping_rounds : 학습 조기 종료를 위한 early stopping interval 값.
  * num_leaves : 최대 리프 노드 갯수
  * min_child_weight : 결정 트리의 min_child_leaf와 유사. 과적합 조절용.



## 공부할 내용

* 시계열 인강 복습할 것